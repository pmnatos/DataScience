{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Decision Tree\n",
    "In this instruction, you will learn how to use python as a powerful tool for data visualization and making decision trees. Note that you do not need to do the requirements section if you have already done the environmental setup. You need to add \"Seaborn_Datasets\" and \"DT_Datasets\" to the home page of your Jupyter notebook.\n",
    "## Requirements\n",
    "Before starting instruction, you should make sure that you have installed the following libraries:\n",
    "- pandas\n",
    "- seaborn\n",
    "- numpy\n",
    "- matplotlib\n",
    "- scipy\n",
    "- sklearn\n",
    "- subprocess\n",
    "- graphviz\n",
    "- p_decision_tree\n",
    "\n",
    "If you have installed the Python virtual environment installation (Instruction 1) you are good to go.\n",
    "\n",
    "If you did not follow the first instruction for Python virtual environment installation then please first follow the first instruction.\n",
    "\n",
    "In general, if you already have installed “Anaconda”, all these libraries should be installed on your system along with “Anaconda” (except \"p_decision_tree\"). You can check the availability of a library by the following command:\n",
    "- python -c \"import {name of library}\"\n",
    "\n",
    "For example to check if “Numpy” is installed or not you can use the following command:\n",
    "- python -c \"import numpy\"\n",
    "\n",
    "In case there is any missing library, you can install it by the following command:\n",
    "- pip install {name of library}\n",
    "    \n",
    "If you have not installed “Graphviz” package version 2.38, in the following link you can find the right package for your operating system.\n",
    "- https://www.graphviz.org/download/\n",
    "\n",
    "After installing graphviz, you need to add the installed executable “dot” path to your system environment path (default path in Windows is “C:\\Program Files\\Graphviz2.38\\bin”).\n",
    "\n",
    "\n",
    "# Data Visualization \n",
    "Data visualization is usually the starting point for data analysis which refers to the graphical representation of data by using visual elements like charts, graphs, maps, etc. In this instruction for data visualization, we will discuss the following topics:\n",
    "1.\tBasic statistical analysis\n",
    "2.\tSimple plots\n",
    "    - Bar plot\n",
    "    - Stacked plot\n",
    "    - Box plot\n",
    "5.\tDistributions\n",
    "    - Plotting univariate distributions\n",
    "    - Plotting bivariate distributions\n",
    "    - Pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyterthemes\n",
      "  Downloading jupyterthemes-0.20.0-py2.py3-none-any.whl (7.0 MB)\n",
      "Requirement already satisfied: notebook>=5.6.0 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from jupyterthemes) (6.1.4)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from jupyterthemes) (4.6.3)\n",
      "Requirement already satisfied: ipython>=5.4.1 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from jupyterthemes) (7.18.1)\n",
      "Collecting lesscpy>=0.11.2\n",
      "  Downloading lesscpy-0.14.0-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from jupyterthemes) (3.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (2.11.2)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.7)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (1.5.0)\n",
      "Requirement already satisfied: nbformat in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.0.8)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.9.1)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.0.4)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.3.4)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (19.0.2)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.0.5)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.0.7)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (20.1.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.8.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from jupyter-core->jupyterthemes) (227)\n",
      "Requirement already satisfied: backcall in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (50.3.0.post20201006)\n",
      "Requirement already satisfied: pygments in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (2.7.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (3.0.8)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.17.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (4.4.2)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.4.4)\n",
      "Collecting ply\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: six in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from lesscpy>=0.11.2->jupyterthemes) (1.15.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.19.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from jinja2->notebook>=5.6.0->jupyterthemes) (1.1.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (3.2.0)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from terminado>=0.8.3->notebook>=5.6.0->jupyterthemes) (0.5.7)\n",
      "Requirement already satisfied: bleach in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (3.2.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.6.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.1.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.4.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.3)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from argon2-cffi->notebook>=5.6.0->jupyterthemes) (1.14.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.4.1->jupyterthemes) (0.2.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from jedi>=0.10->ipython>=5.4.1->jupyterthemes) (0.7.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (20.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (2.0.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (20.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.4.1)\n",
      "Requirement already satisfied: async-generator in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=5.6.0->jupyterthemes) (2.20)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pvnn9\\anaconda3\\envs\\env-ids2020\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (3.4.0)\n",
      "Installing collected packages: ply, lesscpy, jupyterthemes\n",
      "Successfully installed jupyterthemes-0.20.0 lesscpy-0.14.0 ply-3.11\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyterthemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistical Analysis\n",
    "In this section, you will learn how to obtain some basic statistical values like; mean, median, and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing Datasets\n",
    "In Python, some libraries provide some simple datasets for training reasons. Seaborn is one of them that provides several datasets which can be loaded and utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(sns.get_dataset_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always start with taking a look at the data!\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the version of Python that you have installed, you may have access to different datasets. To avoid the inconsistency of the results during the instruction, we use the downloaded datasets in the \"Seaborn_Datasets\" folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "def read_dataset(dataset):\n",
    "    folder = \"Seaborn_Datasets/\"\n",
    "    data = pd.read_csv(folder + dataset)\n",
    "    return data\n",
    "\n",
    "#tips = read_dataset(\"tips.csv\")\n",
    "\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "#Some basic statistics\n",
    "print(np.mean(tips['total_bill'])) #mean for the 'total_bill'\n",
    "print(np.std(tips['total_bill'])) #standard diviation for the 'total_bill' \n",
    "print(np.var(tips['total_bill'])) #variance for the 'total_bill'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "<ol>\n",
    "  <li>Load \"tips.csv\" data set from the datasets folder (Seaborn_Datasets). </li>\n",
    "  <li>Calculate the mean of the \"tip\"s for dinner and lunch.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Plots\n",
    "In this section, you will learn how to draw some simple plots to start data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plot\n",
    "A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. It can show the relationship between a numerical variable and a categorical variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "def read_dataset(dataset):\n",
    "    folder = \"Seaborn_Datasets/\"\n",
    "    data = pd.read_csv(folder + dataset)\n",
    "    return data\n",
    "\n",
    "#tips = read_dataset(\"tips.csv\")\n",
    "\n",
    "sns.barplot(x=\"day\", y=\"tip\", data=tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing estimator to median\n",
    "sns.barplot(x=\"day\", y=\"total_bill\", data=tips, estimator=np.median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Plot\n",
    "A stacked bar graph (or stacked bar chart)  is a chart that uses bars to show comparisons between categories of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#generating data set\n",
    "df = pd.DataFrame(columns=[\"Language\",\"Scripting\", \"Cross Platform\",\"Fast\",\n",
    "                           \"Data Science\",\"Easy\"], \n",
    "                  data=[[\"Python\",1,1,1,1,1],\n",
    "                        [\"Java\",0,1,1,1,0],\n",
    "                        [\"PHP\",1,1,0,0,1],\n",
    "                        [\"Perl\",1,1,1,0,1],\n",
    "                        [\"C#\",0,0,0,0,0]])\n",
    "#drawing stacked bar plot\n",
    "df.set_index('Language').plot(kind='bar', stacked=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot\n",
    "A box plot or boxplot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Random Sample Data (Numpy)\n",
    "Numpy can be used to generate sample data with different distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sample1 = np.random.rand(50) * 100 #Generates 50 random data between [0,100)\n",
    "print(\"Sampl1:\\n\")\n",
    "print(sample1)\n",
    "sample2 = np.ones(25) * 50  #Generates an array with size 25, all values are 50\n",
    "print(\"\\nSample2:\\n\")\n",
    "print(sample2)\n",
    "sample3 = np.random.rand(10) * 100 + 100 #Greater 10 samples bigger than 100\n",
    "sample4 = np.random.rand(10) * -100      #Generate 10 samples between (-100,0]\n",
    "data = np.concatenate((sample1, sample2, sample3, sample4), 0) #Concatenates row by row\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color and shape of outliers\n",
    "plt.boxplot(data, sym='gd') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the orientation (vertical, horizontal)\n",
    "plt.boxplot(data, vert=False, sym='rs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple box plots together\n",
    "data = [data, data[:50],sample1]\n",
    "plt.boxplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "<ol>\n",
    "  <li>Load \"flights.csv\" data set from the datasets folder (Seaborn_Datasets). </li>\n",
    "  <li>Calculate mean, median, and standard deviation of \"passengers\" for the first 100 rows.</li>\n",
    "  <li>Show the average number of passengers per month (bar plot) for the whole data set.</li>\n",
    "  <li>Explore outliers of \"passengers\" (box plot) in the whole data set, is there any outlier?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "The distribution of a statistical data set (or a population) is a listing or function showing all the possible values (or intervals) of the data and how often they occur.\n",
    "### Plotting Univariate Distributions\n",
    "In univariate, the distribution of just one variable is explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    " \n",
    "#sns.set(color_codes=True)\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()\n",
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default distribution with histogram and kernel density\n",
    "sns.displot(data=iris, x='petal_length', kde=True); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without kernel density, with rug plot (small vertical lines show the observations in each bin)\n",
    "sns.displot(data=iris, x='petal_length', kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying number of bins \n",
    "sns.displot(data=iris, x='petal_length', bins=20, kde=False); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without histogram, with rug plot\n",
    "sns.displot(data=iris, x='petal_length', kind='kde', rug=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution functions for subsets of data using hue mapping\n",
    "sns.displot(data=iris, x='petal_length', hue=\"species\", kind='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empirical cumulative distribution functions (ECDFs)\n",
    "sns.displot(data=iris, x='petal_length', kind=\"ecdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cumulative distribution functions for subsets of data using hue mapping\n",
    "sns.displot(data=iris, x='petal_length', hue=\"species\", kind=\"ecdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Bivariate Distributions\n",
    "In this section, we will explore a distribution involving two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean = [10, 20] \n",
    "cov = [(1, .5), (.5, 1)]\n",
    "#Generate 200 random normal data based predefined mean and covariane\n",
    "data = np.random.multivariate_normal(mean, cov, 100)\n",
    "\n",
    "\n",
    "#convert Numpy to Dataframe with specific names for columns \n",
    "df = pd.DataFrame(data, columns=[\"x\", \"y\"])\n",
    "#print(df.corr())\n",
    "\n",
    "sns.jointplot(data=df, x=\"x\", y=\"y\", kind=\"kde\");  #kind= scatter, hex, reg,kde \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing type to Scatter\n",
    "scatter = sns.jointplot(data=df, x=\"x\", y=\"y\", kind=\"scatter\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing type to Hexagons\n",
    "sns.jointplot(data=df, x='x', y='y',kind='hex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing type to Regression\n",
    "sns.jointplot(x=\"x\", y=\"y\", data=df, kind=\"reg\");  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Plot\n",
    "By pair plot, we will create a grid of Axes such that each variable in data will be shared in the y-axis across a single row and in the x-axis across a single column. The diagonal axis shows the univariate distribution of the data for the corresponding variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_dataset(dataset):\n",
    "    folder = \"Seaborn_Datasets/\"\n",
    "    data = pd.read_csv(folder + dataset)\n",
    "    return data\n",
    "\n",
    "#iris = read_dataset(\"iris.csv\")\n",
    "iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default all numeric variables are used\n",
    "sns.pairplot(iris); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify specific variables\n",
    "sns.pairplot(iris, vars = ['petal_length','sepal_length']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Color\n",
    "sns.pairplot(iris, hue = 'species');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding markers\n",
    "sns.pairplot(iris, hue = 'species', markers=[\"o\", \"s\", \"D\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernel density estimation, \n",
    "sns.pairplot(iris, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn :)\n",
    "<ol>\n",
    "  <li>Load \"mpg\" data set by seaborn. </li>\n",
    "  <li>Show distribution of \"horsepower\" and \"acceleration\" together (by a joint plot). Interpret the correlation between \"horsepower\" and \"acceleration\".</li>\n",
    "  <li>Compare the correlation between \"horsepower\", \"weight\", and \"acceleration\" for the cars produced by different continents (\"origin\"). </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "In this part, we will use the \"p_decision_tree\" library to make a decision tree based on categorical descriptive attributes and the \"scikit-learn\" library to make a decision tree based on numerical descriptive attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree (Categorical Descriptive Attributes)\n",
    "We use the \"p_decision_tree\" library to make a decision tree based on the categorical descriptive attributes (make sure that you have installed \"p_decision_tree\" library). This library is not able to make a decision tree based on the numerical descriptive attributes, and you have to convert the numerical descriptive attributes to the categorical attributes. \n",
    "\n",
    "Note that in order to see a visual tree, you need to install graphviz package. [Here](https://www.graphviz.org/download/) you can find the right package with respect to your operating system. \n",
    "### Features\n",
    "The main algorithm used by the library is ID3 with the following features:\n",
    "\n",
    "* Information gain based on [entropy](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "* Information gain based on [gini](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "* Some pruning capabilities like:\n",
    "\t* Minimum number of samples\n",
    "\t* Minimum information gain\n",
    "* The resulting tree is not binary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "As aforementioned, you can simply load “csv” or “excel” data by the corresponding methods (“read_csv”, “read_excel” respectively) of Pandas. Make sure that you have uploaded the \"DT_Datasets\" folder on the home page of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p_decision_tree.DecisionTree import DecisionTree\n",
    "import pandas as pd\n",
    "\n",
    "def read_dataset(dataset):\n",
    "    folder = \"DT_Datasets/\"\n",
    "    data = pd.read_csv(folder + dataset)\n",
    "    return data\n",
    "\n",
    "data = read_dataset('playtennis.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Descriptive and Target Attributes (Features)\n",
    "As you know based on the concepts of decision tree descriptive features and target feature should be specified. Descriptive features are used to make a decision to predict the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "\n",
    "#All columns except the last one are descriptive by default\n",
    "descriptive_features = columns[:-1]\n",
    "#The last column is considered as label\n",
    "label = columns[-1]\n",
    "\n",
    "#Converting all the columns to string\n",
    "for column in columns:\n",
    "    data[column]= data[column].astype(str)\n",
    "\n",
    "data_descriptive = data[descriptive_features].values\n",
    "data_label = data[label].values\n",
    "\n",
    "print(\"descriptive features:\")\n",
    "print(descriptive_features)\n",
    "print(\"\\ntarget feature:\\n\" + label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree (Numerical Descriptive Attributes)\n",
    "The \"scikit-learn\" library is used to make a decision tree based on numerical descriptive attributes. Note that \"scikit-learn\" as the main library for data science in Python is not able to make a decision tree based on categorical descriptive attributes, and you have to convert the categorical attributes to numerical before passing them to the classifier method. Also, the resulting decision tree by this library is a binary tree.\n",
    "In the following, you can find a sample code to make a decision tree based on numerical descriptive attributes, using \"scikit-learn\" library.\n",
    "\n",
    "“DecisionTreeClassifier” method of “sklearn” is used to generate the tree classifier. You can set the parameters of this method based on what you need. In the following you can find some of the most important parameters of this method:\n",
    "- Main parameters to specify the algorithm\n",
    "    - Criterion: The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain. (Default = \"gini\")\n",
    "    - Splitter: The strategy used to choose the split at each node. Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split. (Default = \"best\")\n",
    "- Parameters to control the growth of the tree (Pruning)\n",
    "    - Min_samples_split: The minimum number of samples required to split an internal node\n",
    "    - Min_samples_leaf: The minimum number of samples required to be at a leaf node. (Default = 1)\n",
    "    - Max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than “min_samples_split” samples. (Default = None)\n",
    "    - Max_leaf_nodes: Grow a tree with “max_leaf_nodes” in the best-first fashion. Best nodes are defined as relative reduction in impurity. If None, then an unlimited number of leaf nodes. (Default = None)\n",
    "    - Min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value. (Default = 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Tree\n",
    "The \"id3\" method is used to make the decision tree. One can pass the minimum gain and also the minimum samples to this function to prune the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calling DecisionTree constructor (the last parameter is criterion which can also be \"gini\")\n",
    "decisionTree = DecisionTree(data_descriptive.tolist(), descriptive_features.tolist(), data_label.tolist(), \"entropy\")\n",
    "\n",
    "#Here you can pass pruning features (gain_threshold and minimum_samples)\n",
    "decisionTree.id3(0,0)\n",
    "\n",
    "#Visualizing decision tree by Graphviz\n",
    "dot = decisionTree.print_visualTree( render=True )\n",
    "\n",
    "#print(dot)\n",
    "\n",
    "print(\"System entropy: \", format(decisionTree.entropy))\n",
    "print(\"System gini: \", format(decisionTree.gini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from subprocess import check_output\n",
    "\n",
    "#loading dataset\n",
    "def read_dataset(dataset):\n",
    "    folder = \"DT_Datasets/\"\n",
    "    data = pd.read_csv(folder + dataset)\n",
    "    return data\n",
    "\n",
    "data = read_dataset('ManWoman.csv')\n",
    "\n",
    "#descriptive features\n",
    "X = data[['height','weight']] \n",
    "#target feature\n",
    "Y = data[[\"Class\"]]\n",
    "\n",
    "\n",
    "job_classifier = tree.DecisionTreeClassifier(criterion=\"entropy\")   \n",
    "job_classifier.fit(X, Y)\n",
    "\n",
    "\n",
    "column_names = list(data.columns.values)\n",
    "del column_names[-1]\n",
    "dot_file = \"Classification.dot\"\n",
    "pdf_file = \"Classification.pdf\"\n",
    "with open(dot_file, \"w\") as f:\n",
    "    f = tree.export_graphviz(job_classifier, out_file=f, \n",
    "                                 feature_names= column_names, \n",
    "                                 class_names=[\"Man\", \"Woman\"], \n",
    "                                 filled=True, rounded=True)\n",
    "try:\n",
    "    check_output(\"dot -Tpdf \"+ dot_file + \" -o \" + pdf_file , shell=True)\n",
    "    print(\"Find Classification.dot (description) and Classification.pdf (visual tree) in the home page of your Jupyter.\")\n",
    "except:\n",
    "    print(\"Make sure that you have installed Graphviz, otherwise you can not see the visual tree. But you can find descriptions in a dot file\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "©PADS-RWTH (use only with permission & acknowledgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
